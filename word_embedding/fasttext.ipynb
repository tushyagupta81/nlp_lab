{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041d4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5682094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, window_size, embedding_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, target):\n",
    "        target_embedding = self.embeddings(target)\n",
    "        res = self.linear(target_embedding)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c33753",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "doc = [\n",
    "    \"<i am henry>\",\n",
    "    \"<i like college>\",\n",
    "    \"<do henry like college>\",\n",
    "    \"<i am do i like college>\",\n",
    "    \"<i do like henry>\",\n",
    "    \"<do i like henry>\",\n",
    "]\n",
    "raw_text = \" \".join(doc)\n",
    "tokens = raw_text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de533c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_tokens(tok):\n",
    "    char_tokens = []\n",
    "    for token in tok:\n",
    "        if len(token) < window_size:\n",
    "            char_tokens.append(token)\n",
    "        for i in range(0, len(token) - window_size + 1):\n",
    "            char_tokens.append(token[i : i + window_size])\n",
    "    return char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e0a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<i', 'am', 'hen', 'enr', 'nry', 'ry>', '<i', 'lik', 'ike', 'col', 'oll', 'lle', 'leg', 'ege', 'ge>', '<do', 'hen', 'enr', 'nry', 'lik', 'ike', 'col', 'oll', 'lle', 'leg', 'ege', 'ge>', '<i', 'am', 'do', 'i', 'lik', 'ike', 'col', 'oll', 'lle', 'leg', 'ege', 'ge>', '<i', 'do', 'lik', 'ike', 'hen', 'enr', 'nry', 'ry>', '<do', 'i', 'lik', 'ike', 'hen', 'enr', 'nry', 'ry>']\n"
     ]
    }
   ],
   "source": [
    "char_tokens = get_new_tokens(tokens)\n",
    "print(char_tokens)\n",
    "vocab = set(char_tokens)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e4246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 1), (14, 3), (14, 15), (14, 5), (14, 2), (14, 1), (5, 3), (5, 15), (5, 14), (5, 2), (5, 1), (5, 6), (2, 15), (2, 14), (2, 5), (2, 1), (2, 6), (2, 4), (1, 14), (1, 5), (1, 2), (1, 6), (1, 4), (1, 0), (6, 5), (6, 2), (6, 1), (6, 4), (6, 0), (6, 7), (4, 2), (4, 1), (4, 6), (4, 0), (4, 7), (4, 8), (0, 1), (0, 6), (0, 4), (0, 7), (0, 8), (0, 16), (7, 6), (7, 4), (7, 0), (7, 8), (7, 16), (7, 11), (8, 4), (8, 0), (8, 7), (8, 16), (8, 11), (8, 13), (16, 0), (16, 7), (16, 8), (16, 11), (16, 13), (16, 9), (11, 7), (11, 8), (11, 16), (11, 13), (11, 9), (11, 15), (13, 8), (13, 16), (13, 11), (13, 9), (13, 15), (13, 14), (9, 16), (9, 11), (9, 13), (9, 15), (9, 14), (9, 5), (15, 11), (15, 13), (15, 9), (15, 14), (15, 5), (15, 6), (14, 13), (14, 9), (14, 15), (14, 5), (14, 6), (14, 4), (5, 9), (5, 15), (5, 14), (5, 6), (5, 4), (5, 0), (6, 15), (6, 14), (6, 5), (6, 4), (6, 0), (6, 7), (4, 14), (4, 5), (4, 6), (4, 0), (4, 7), (4, 8), (0, 5), (0, 6), (0, 4), (0, 7), (0, 8), (0, 16), (7, 6), (7, 4), (7, 0), (7, 8), (7, 16), (7, 11), (8, 4), (8, 0), (8, 7), (8, 16), (8, 11), (8, 13), (16, 0), (16, 7), (16, 8), (16, 11), (16, 13), (16, 1), (11, 7), (11, 8), (11, 16), (11, 13), (11, 1), (11, 3), (13, 8), (13, 16), (13, 11), (13, 1), (13, 3), (13, 12), (1, 16), (1, 11), (1, 13), (1, 3), (1, 12), (1, 10), (3, 11), (3, 13), (3, 1), (3, 12), (3, 10), (3, 6), (12, 13), (12, 1), (12, 3), (12, 10), (12, 6), (12, 4), (10, 1), (10, 3), (10, 12), (10, 6), (10, 4), (10, 0), (6, 3), (6, 12), (6, 10), (6, 4), (6, 0), (6, 7), (4, 12), (4, 10), (4, 6), (4, 0), (4, 7), (4, 8), (0, 10), (0, 6), (0, 4), (0, 7), (0, 8), (0, 16), (7, 6), (7, 4), (7, 0), (7, 8), (7, 16), (7, 11), (8, 4), (8, 0), (8, 7), (8, 16), (8, 11), (8, 13), (16, 0), (16, 7), (16, 8), (16, 11), (16, 13), (16, 1), (11, 7), (11, 8), (11, 16), (11, 13), (11, 1), (11, 12), (13, 8), (13, 16), (13, 11), (13, 1), (13, 12), (13, 6), (1, 16), (1, 11), (1, 13), (1, 12), (1, 6), (1, 4), (12, 11), (12, 13), (12, 1), (12, 6), (12, 4), (12, 15), (6, 13), (6, 1), (6, 12), (6, 4), (6, 15), (6, 14), (4, 1), (4, 12), (4, 6), (4, 15), (4, 14), (4, 5), (15, 12), (15, 6), (15, 4), (15, 14), (15, 5), (15, 2), (14, 6), (14, 4), (14, 15), (14, 5), (14, 2), (14, 9), (5, 4), (5, 15), (5, 14), (5, 2), (5, 9), (5, 10), (2, 15), (2, 14), (2, 5), (2, 9), (2, 10), (2, 6), (9, 14), (9, 5), (9, 2), (9, 10), (9, 6), (9, 4), (10, 5), (10, 2), (10, 9), (10, 6), (10, 4), (10, 15), (6, 2), (6, 9), (6, 10), (6, 4), (6, 15), (6, 14), (4, 9), (4, 10), (4, 6), (4, 15), (4, 14), (4, 5), (15, 10), (15, 6), (15, 4), (15, 14), (15, 5), (15, 2)]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "word_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "for i in range(window_size, len(char_tokens) - window_size):\n",
    "    context = word_index[char_tokens[i]]\n",
    "    for j in range(-window_size, window_size + 1):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        data.append((context, word_index[char_tokens[i + j]]))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa214dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "model = SkipGram(vocab_size, window_size, embed_size)\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2756d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.8977245055899328\n",
      "50 2.3386322324778757\n",
      "100 2.287954709156841\n",
      "150 2.2673744076774236\n",
      "200 2.256828162540384\n",
      "250 2.2503388248333316\n",
      "300 2.2462501558316808\n",
      "350 2.243549324217297\n",
      "400 2.241594126435364\n",
      "450 2.2400413674562154\n",
      "500 2.238715876932858\n",
      "550 2.237530699798039\n",
      "600 2.2364458295763754\n",
      "650 2.235446527296183\n",
      "700 2.234530120885291\n",
      "750 2.2336968061875324\n",
      "800 2.232944846964207\n",
      "850 2.232268684575347\n",
      "900 2.2316595356480606\n",
      "950 2.231107618938498\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.tensor([context]))\n",
    "        loss = lossfn(output, torch.tensor([target]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(epoch, total_loss / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6462ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'hen': [[-0.79532444 -0.96615595  0.6535394  -1.0637437  -0.02516011  1.9169487\n",
      "  -0.06009363  0.67181677 -0.22680724 -0.5015251 ]]\n",
      "Embedding for 'enr': [[-0.43325138 -0.20698804  0.16642267 -0.58563775  1.3359994   0.58535695\n",
      "  -1.5445085  -1.1151849  -0.49175018  0.8841796 ]]\n",
      "Embedding for 'nry': [[ 0.42004493  1.4199466   0.78154904 -0.18266961 -0.2868884   0.35812554\n",
      "  -0.64863205  0.87368417 -0.8444467   1.1747092 ]]\n",
      "Embedding for henbenry: [-0.2695103   0.08226752  0.533837   -0.6106837   0.34131697  0.95347697\n",
      " -0.75107807  0.14343868 -0.5210014   0.5191212 ]\n"
     ]
    }
   ],
   "source": [
    "word_to_lookup = \"henbenry\"\n",
    "lookup = get_new_tokens([word_to_lookup])\n",
    "res = []\n",
    "for lu in lookup:\n",
    "    if lu in word_index.keys():\n",
    "        wi = word_index[lu]\n",
    "        embedding = model.embeddings(torch.tensor([wi]))\n",
    "        res.append(embedding.detach().numpy()[0])\n",
    "        print(f\"Embedding for '{lu}': {embedding.detach().numpy()}\")\n",
    "\n",
    "print(f\"Embedding for {word_to_lookup}: {sum(res) / len(res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72b859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
