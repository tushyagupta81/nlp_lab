{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddeff71a",
   "metadata": {},
   "source": [
    "# FastText\n",
    "FastText is a word embedding technique that breaks down words into n grams and creates the word embeddings for those smaller units\\\n",
    "This helps FastText to also find out ebmedding for previously unseen words by using the smaller parts of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041d4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5682094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, window_size, embedding_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, target):\n",
    "        target_embedding = self.embeddings(target)\n",
    "        res = self.linear(target_embedding)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c33753",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "doc = [\n",
    "    \"<i am henry>\",\n",
    "    \"<i like college>\",\n",
    "    \"<do henry like college>\",\n",
    "    \"<i am do i like college>\",\n",
    "    \"<i do like henry>\",\n",
    "    \"<do i like henry>\",\n",
    "]\n",
    "raw_text = \" \".join(doc)\n",
    "tokens = raw_text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de533c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_tokens(tok):\n",
    "    char_tokens = []\n",
    "    for token in tok:\n",
    "        if len(token) < window_size:\n",
    "            char_tokens.append(token)\n",
    "        for i in range(0, len(token) - window_size + 1):\n",
    "            char_tokens.append(token[i : i + window_size])\n",
    "    return char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e0a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<i', 'am', 'hen', 'enr', 'nry', 'ry>', '<i', 'lik', 'ike', 'col']\n"
     ]
    }
   ],
   "source": [
    "char_tokens = get_new_tokens(tokens)\n",
    "print(char_tokens[:10])\n",
    "vocab = set(char_tokens)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e4246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 16), (3, 0), (3, 1), (3, 13), (3, 14), (3, 16), (13, 0), (13, 1), (13, 3), (13, 14)]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "word_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "for i in range(window_size, len(char_tokens) - window_size):\n",
    "    context = word_index[char_tokens[i]]\n",
    "    for j in range(-window_size, window_size + 1):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        data.append((context, word_index[char_tokens[i + j]]))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa214dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "model = SkipGram(vocab_size, window_size, embed_size)\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2756d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.929987609791918\n",
      "50 2.3488601352892764\n",
      "100 2.2823653622549407\n",
      "150 2.261180039165782\n",
      "200 2.253301527629904\n",
      "250 2.249035820263584\n",
      "300 2.2461052427486496\n",
      "350 2.2436619603714973\n",
      "400 2.241289444521171\n",
      "450 2.2389877912949543\n",
      "500 2.2370689385602263\n",
      "550 2.235602776614987\n",
      "600 2.234455091207206\n",
      "650 2.2335173297090596\n",
      "700 2.2327251633008323\n",
      "750 2.2320393364445694\n",
      "800 2.2314340431673996\n",
      "850 2.230892019206975\n",
      "900 2.230401324982546\n",
      "950 2.2299533826964244\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.tensor([context]))\n",
    "        loss = lossfn(output, torch.tensor([target]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(epoch, total_loss / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6462ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'hen': [[-0.8204228   0.10082058 -0.7994618  -0.63440734 -0.32108185  0.3642752\n",
      "   0.25619638  0.5068886   2.0844624  -0.01407014]]\n",
      "Embedding for 'enr': [[-0.424241   -0.6773601  -0.7903334   0.26817885  0.25904417  1.2208512\n",
      "  -0.5175885   1.9262754  -0.9821287  -0.88805467]]\n",
      "Embedding for 'nry': [[ 0.29178938 -2.0117695  -0.89259404 -1.5418677  -0.04163222 -0.38947254\n",
      "  -0.5865991  -0.7319726  -1.0225619  -0.28157222]]\n",
      "Embedding for henbenry: [-0.3176248  -0.86276966 -0.8274631  -0.63603204 -0.03455663  0.39855132\n",
      " -0.28266373  0.56706387  0.02659062 -0.39456567]\n"
     ]
    }
   ],
   "source": [
    "word_to_lookup = \"henbenry\"\n",
    "lookup = get_new_tokens([word_to_lookup])\n",
    "res = []\n",
    "for lu in lookup:\n",
    "    if lu in word_index.keys():\n",
    "        wi = word_index[lu]\n",
    "        embedding = model.embeddings(torch.tensor([wi]))\n",
    "        res.append(embedding.detach().numpy()[0])\n",
    "        print(f\"Embedding for '{lu}': {embedding.detach().numpy()}\")\n",
    "\n",
    "print(f\"Embedding for {word_to_lookup}: {sum(res) / len(res)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
