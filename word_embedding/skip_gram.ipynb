{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de2dc6d",
   "metadata": {},
   "source": [
    "# Skip-Gram\n",
    "This is a word embedding technique under the Word2Vec method\\\n",
    "Here we use a shallow neural netword to create our word embeddings\\\n",
    "We feed in a single word to the NN to predict the surrouding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9070b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f3bba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, window_size, embedding_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, target):\n",
    "        target_embedding = self.embeddings(target)\n",
    "        res = self.linear(target_embedding)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5233eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "doc = [\n",
    "    \"i am henry\",\n",
    "    \"i like college\",\n",
    "    \"do henry like college\",\n",
    "    \"i am do i like college\",\n",
    "    \"i do like henry\",\n",
    "    \"do i like henry\",\n",
    "]\n",
    "raw_text = \" \".join(doc)\n",
    "tokens = raw_text.split()\n",
    "vocab = set(tokens)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0380535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 5), (1, 4), (1, 5), (1, 2), (5, 4), (5, 1), (5, 2), (5, 3), (2, 1), (2, 5), (2, 3), (2, 0), (3, 5), (3, 2), (3, 0), (3, 1), (0, 2), (0, 3), (0, 1), (0, 2), (1, 3), (1, 0), (1, 2), (1, 3), (2, 0), (2, 1), (2, 3), (2, 5), (3, 1), (3, 2), (3, 5), (3, 4), (5, 2), (5, 3), (5, 4), (5, 0), (4, 3), (4, 5), (4, 0), (4, 5), (0, 5), (0, 4), (0, 5), (0, 2), (5, 4), (5, 0), (5, 2), (5, 3), (2, 0), (2, 5), (2, 3), (2, 5), (3, 5), (3, 2), (3, 5), (3, 0), (5, 2), (5, 3), (5, 0), (5, 2), (0, 3), (0, 5), (0, 2), (0, 1), (2, 5), (2, 0), (2, 1), (2, 0), (1, 0), (1, 2), (1, 0), (1, 5), (0, 2), (0, 1), (0, 5), (0, 2), (5, 1), (5, 0), (5, 2), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "word_index = {word: i for i, word in enumerate(vocab)}\n",
    "# onehot = {word: [0 for _ in vocab] for word in vocab}\n",
    "# for i, word in enumerate(vocab):\n",
    "#     onehot[word][i] = 1\n",
    "\n",
    "for i in range(2, len(tokens) - 2):\n",
    "    context = word_index[tokens[i]]\n",
    "    for j in range(-window_size, window_size + 1):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        data.append((context, word_index[tokens[i + j]]))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11dca4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "model = SkipGram(vocab_size, window_size, embed_size)\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca0edd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8511932469904422\n",
      "50 1.545641028881073\n",
      "100 1.5361952051520347\n",
      "150 1.5330601558089256\n",
      "200 1.5311076186597348\n",
      "250 1.529517511278391\n",
      "300 1.5280845649540424\n",
      "350 1.5267456322908401\n",
      "400 1.5254790008068084\n",
      "450 1.524276676028967\n",
      "500 1.5231351152062416\n",
      "550 1.5220524609088897\n",
      "600 1.5210270047187806\n",
      "650 1.5200570434331895\n",
      "700 1.5191406615078449\n",
      "750 1.518275821954012\n",
      "800 1.5174602322280406\n",
      "850 1.5166916601359843\n",
      "900 1.515967819094658\n",
      "950 1.5152863003313541\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.tensor([context]))\n",
    "        loss = lossfn(output, torch.tensor([target]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(epoch, total_loss / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "beff3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'henry': [[-1.7774011   1.436296   -0.60040134  1.001669    0.47328883  0.277007\n",
      "  -1.6477649   1.3790239   0.685      -0.6399059 ]]\n"
     ]
    }
   ],
   "source": [
    "word_to_lookup = \"henry\"\n",
    "wi = word_index[word_to_lookup]\n",
    "embedding = model.embeddings(torch.tensor([wi]))\n",
    "print(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
